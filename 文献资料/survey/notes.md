# A Survey of Large Language Models

![[【脑图】A survey og Large Language Models.jpeg]]

## 摘要

自20世纪50年代提出图灵测试以来，人类一直在探索机器掌握语言智能。语言本质上是一种由语法规则治理的人类表达的复杂而精细系统。开发能够理解并掌握一门语言的有能力的人工智能（AI）算法是一个重大挑战。作为主要方法之一，语言建模在过去二十年中广泛研究了语言理解和生成，在统计语言模型的基础上发展到神经语言模型。最近，研究人员提出了通过预训练Transformer模型在大规模语料库上进行预训练的语言模型（PLMs），这些模型在解决各种自然语言处理（NLP）任务时表现出强大的能力。由于发现模型规模可以提高模型容量，研究人员进一步通过增加参数规模来研究规模效应。有趣的是，当参数规模超过一定水平时，这些扩大的语言模型不仅实现了显著性能改进，而且还显示出一些特殊的能力（例如，在上下文学习），这是小型语言模型（例如BERT）所不具备的。为了区分不同参数规模的语言模型，研究社区将具有大量参数（例如包含数十亿或数百亿个参数）的PLMs称为大型语言模型（LLM）。近年来，学术界和工业界对LLMs的研究取得了长足进展，并且一个令人瞩目的进步是ChatGPT（基于LLMs开发的强大AI聊天机器人）的发布，这引起了社会的广泛关注。LLMs的技术演进正在深刻影响整个AI社区，它将改变我们开发和使用AI算法的方式。考虑到这一快速的技术进步，本文介绍了背景、关键发现和技术主流，重点介绍了四个主要方面：预训练、适应性调整、利用和容量评估。此外，我们也总结了开发LLMs可用的资源，并讨论了未来方向中存在的问题。本文提供了一篇关于LLMs文献的最新综述，这对于研究人员和工程师来说都是一份有用的参考资料。 

## 笔记

